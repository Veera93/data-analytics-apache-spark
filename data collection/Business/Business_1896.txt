 SAN FRANCISCO — Google’s YouTube is hiring more humans to teach machines how to think like humans. In a blog post from Susan Wojcicki, YouTube’s chief executive, the company said on Monday that it planned to add thousands of human reviewers to remove videos that violated its guidelines while teaching computers how to spot troublesome videos. YouTube plans to have 10,000 people dedicated to reviewing videos in 2018 — though it would not to say how many workers it has doing that job now. The hiring spree comes YouTube is mired, yet again, in controversy over failing to properly police content uploaded to its site. The latest batch of problematic videos, reported earlier by The Times of London, are videos of children in states of undress, with comments from pedophiles attracted to the content. Those videos also had advertisements running with them, prompting marketers to pull their ads. Last week, YouTube said it took down more than 150,000 videos featuring children and disabled comments for more than 625,000 videos. It also kicked several hundred YouTube users off the platform for posting “predatory comments on videos featuring minors.” This comes on the heels of a New York Times report about how inappropriate videos featuring children’s cartoon characters in violent or lewd situations slipped past its filters and appeared on YouTube Kids, an app that is supposed to present only child-friendly videos. “I’ve also seen up-close that there can be another, more troubling, side of YouTube’s openness,” Ms. Wojcicki wrote. “I’ve seen how some bad actors are exploiting our openness to mislead, manipulate, harass or even harm.” Because it is easy to upload videos to the site, YouTube has become a repository for all types of niche and mainstream content — replacing television as the primary destination for a younger audience. However, it has also drawn a range of objectionable material, from videos promoting conspiracy theories to violent extremists. More than 400 hours of videos are uploaded to YouTube every minute, and company executives have long said that monitoring that fire hose of content is difficult. They contend that computers — learning from the example of humans — are the answer. YouTube said it had been successful in catching extremist videos using so-called machine learning. Since YouTube laid out new policies aimed at curbing extremist videos on the platform after the London Bridge attacks in June, the company said its machines have improved at catching extremist videos shortly after they were uploaded. YouTube said it has taken down 150,000 videos for violent extremism — in large part because of machines flagging more videos for human reviewers to take down. The video service said it planned to take a similar approach to tackling videos unsafe for children and content featuring hate speech. When it comes to advertising, YouTube said it would take a “new approach.” It plans to consider which channels and videos are eligible for advertising, applying stricter criteria with more human curation and oversight. Ms. Wojcicki did not lay out specific changes, but said YouTube planned to speak to advertisers and content creators in the coming weeks to hone its approach.