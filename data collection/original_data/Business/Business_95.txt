 YouTube Kids, which has been criticized for inadvertently recommending disturbing videos to children, said Wednesday that it would introduce several ways for parents to limit what can be watched on the popular app. Beginning this week, parents will be able to select “trusted channels” and topics that their children can access on the app, like “Sesame Workshop” or “learning,” that have been curated by people at YouTube Kids and its partners. The Google-owned app said in a blog post on Wednesday that parents would also have the option to restrict video recommendations to channels that have been “verified” by YouTube Kids, avoiding the broader sea of content that the app pulls from the main YouTube site through algorithms and other automated processes. YouTube Kids was introduced in 2015 for children of preschool age and older, and it says it has more than 11 million weekly viewers. But parents have discovered a range of inappropriate videos on the app, highlighting the platform’s dependence on automation and a lack of human oversight. The New York Times reported in the fall that children using the app had been shown videos with popular characters from Nick Jr. and Disney Junior in violent or lewd situations, and other disturbing imagery, sometimes set to nursery rhymes. More recently, Business Insider reported that the app was suggesting conspiracy theory videos to children, including claims that the world is flat and that the moon landing was faked. YouTube Kids had previously relied primarily on parents to report troubling videos, a practice that was criticized because children are often the only ones watching the content on tablets or phones. Later this year, parents will be able to handpick every video and channel that children can view through the app, YouTube Kids said in its blog post, even restricting them to, say, 10 videos or a single channel. The changes announced Wednesday provide “a more robust suite of tools for parents to customize the YouTube Kids experience,” James Beser, product director for the platform, said in a statement. “From collections of channels from trusted partners to enabling parents to select each video and channel themselves, we’re putting parents in the driver’s seat like never before.” The announcement is part of a broader conversation around how Google, the world’s biggest seller of digital advertising, uses human oversight for the enormous volume of content on YouTube. CNN recently found hundreds of advertisers, including The Times, on inappropriate YouTube channels promoting conspiracies, white nationalism and pedophilia. YouTube has said it will be using more human reviewers, particularly with Google Preferred, a grouping of top-tier videos on the site that helps brands advertise on popular videos. YouTube has faced other issues around children. This month, a coalition of consumer advocacy groups contended that YouTube had been aware that children 12 and younger have been watching the main site and that, therefore, it had been violating a privacy law that requires parental consent before collecting and mining their data. Google assumes that viewers on the main YouTube site and app are at least 13 years old, which enables the company to collect certain information from those users and serve them personalized ads. YouTube said it planned to read the complaint “thoroughly” but noted that “because YouTube is not for children, we’ve invested significantly in the creation of the YouTube Kids app to offer an alternative specifically designed for children.”